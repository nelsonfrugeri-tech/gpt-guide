
### CREATE COMPLETION

> Given a prompt, the model will return one or more predicted completions, and can also return the probabilities of alternative tokens at each position.

`POST https://api.openai.com/v1/completions`

Creates a completion for the provided prompt and parameters

| **Field** | **What's it?** | **Examples** | 
| --- | --- | -- |
| engine | The "engine" parameter in the create method of the OpenAI Chat Completion API is used to specify which language model should be used to generate the response for the user input provided. Each language model is trained on a different set of data and can be specialized for specific tasks. | 1. "davinci": This model is considered the most advanced and generally produces more accurate and coherent responses. It is trained on a wide variety of data and is useful for general text generation tasks. <br> 2. "curie": This model is specialized in text generation tasks for specific purposes, such as summaries, translations, and product descriptions. <br> 3. "babbage": This model is less accurate than the others, but it is faster and can be used for tasks that require less precision, such as simple chatbots. | 
| prompt | It's the parameter in the OpenAI Chat Completion API is used to specify the input text or prompt for which the API should generate a response. It is the text that is used to seed the AI model's response. | Here are some examples of prompts that could be used with the API: <br> - "Can you write me a short story about a haunted house?" <br> - "Can you provide a summary of the latest research on climate change?" <br> - "Can you generate a list of potential names for a new restaurant?" <br> - "Can you translate the phrase 'hello, how are you?' from English to French?" <br> - "Can you write a product description for a new smartphone?" <br> The API will then use the provided prompt to generate a response that is relevant to the given task. |
| max_tokens | The parameter "max_tokens" specifies the maximum number of tokens (words or subwords) that the API can generate in the response. | 1. Limiting the length of generated text: By setting a maximum value for the max_tokens parameter, you can limit the length of the generated text. For example, if you only want a short response, you can set max_tokens to 50, which will ensure that the API generates a response that is no longer than 50 tokens. <br> 2. Controlling the level of detail: If you want the generated text to be more detailed, you can increase the max_tokens value to allow for a longer response. For example, if you want a detailed description of a product, you can set max_tokens to 200, which will allow for a longer response with more details. |
| temperature | The "temperature" parameter in OpenAI's API Chat Completion method determines the level of creativity or randomness in the generated text. A lower temperature value results in more conservative and predictable text, while a higher value allows for more creative and unpredictable responses. The purpose of this parameter is to control the level of randomness in the generated text, and to allow for customization of the generated text based on the desired level of creativity. | <b>1. Low temperature:</b> temperature = 0.2 <br> Example prompt: "Can you generate a list of potential book titles?" Generated response: "1. The Last Voyage of the Sun 2. The River of Shadows 3. The Hidden Kingdom" Explanation: With a low temperature value, the generated response tends to be more conservative and less creative, often repeating similar phrases and structures. <br> <b>2. High temperature:</b> temperature = 1.0 Example prompt: "Can you write a short story about a robot who gains sentience?" Generated response: "I am alive! The robot thought. It was a feeling he had never experienced before. He looked around the room, taking in all the details of his surroundings. He was fascinated by everything he saw." Explanation: With a high temperature value, the generated response tends to be more unpredictable and creative, often producing unexpected or even nonsensical results. <br> <b>3. Medium temperature:</b> temperature = 0.5. Example prompt: "Can you provide a summary of the latest research on COVID-19 vaccines?" Generated response: "Recent studies have shown that the COVID-19 vaccines are highly effective at preventing severe illness, hospitalization, and death. They also offer some level of protection against the new variants of the virus that have emerged in recent months." Explanation: With a medium temperature value, the generated response strikes a balance between being conservative and creative, often producing responses that are more varied and nuanced. |
| top_p | The "top_p" parameter, also known as "nucleus sampling", is another OpenAI GPT API parameter which can be used to control the diversity of the responses generated by the model. This parameter allows the model to choose the most likely words to be used in the response, based on an accumulative probability, ranging from 0 to 1. | For example, if you set the value of the "top_p" parameter to 0.9, the model will choose the most likely words, whose cumulative probability adds up to 0.9, to generate the response. This can lead to more diverse and creative answers than using the "temperature" parameter alone. <br> The main advantage of the "top_p" parameter is that it helps avoid "off-topic" answers because, unlike "temperature", "top_p" does not choose words randomly but rather based on a cumulative probability. <b> In terms of context, you can adjust the value of the "top_p" parameter according to the chatbot's goal and the type of question the user asked. For example, for more complex or open-ended questions, you can use a higher value of "top_p" to generate more creative and varied responses. For simpler or factual questions, you can use a lower "top_p" value to ensure more accurate and relevant answers. <br> It is important to remember that the "top_p" parameter should be used in conjunction with the "temperature" parameter to achieve better results in the diversity and creativity of the answers generated by the model. In addition, it is recommended to experiment with different values of "top_p" and "temperature" and observe how the responses generated by the model are affected in different contexts. |
| n | The "n" parameter is an optional parameter in the OpenAI GPT API that controls the number of alternative responses generated by the template. This parameter specifies how many different responses the model should generate and return to the user in a single request. | By default the value of the "n" parameter is set to 1 in the OpenAI GPT API. This means that the model generates a single response for each request. However, you can increase the value of "n" to generate more alternative responses. <br> By using the "n" parameter in your chatbot, you can provide multiple response options to the user so that they can choose the one that best fits their question or need. However, it is important to remember that not all answers generated by the model may be suitable or relevant to the user's question, and that the optimal value for the "n" parameter will depend on the type of chatbot and the context of the conversation. <br> Also, it is important to remember that when using the "n" parameter, the API response will return a list of alternative answers rather than a single answer. If you are using a text-based chatbot, you will need to adjust your programming logic to present the alternative answers to the user and allow them to choose one of them. <br> In summary, the "n" parameter can be used in a chatbot to generate multiple response options for the user and allow for greater interaction and personalization of the conversation. However, it is important to experiment with different values of "n" and observe how the responses generated by the model are affected in different contexts. |
| stop | The "stop" parameter is used in the OpenAI Completions API to tell the model when to stop generating text. The value assigned to the "stop" parameter is a text that the model should use as a stop signal. When the model encounters the indicated text, it stops generating any more words. | Generate a limited response: Let's say you are building a chatbot for customer service and you want the bot to give a short and objective response. You can use the "stop" parameter to tell the model to stop generating text as soon as it provides a sufficiently complete answer. For example, if the user asks "What are the store hours?", you can use the "stop" parameter to tell the model to stop generating text as soon as it provides the hours of operation. <br> Generate multiple options: say you are building a text generation template to help users write titles for their blog posts. You can use the "stop" parameter to generate several different title options. For example, you can provide several ending options, such as " |", "!", "?", and ".", so that the template can generate several different titles, stopping generating text as soon as it encounters one of the ending options. <br> To limit the number of responses returned by OpenAI's Chat Completion, you can set the value of the "stop" parameter to 10, since you want your chat to return only the top 10 movies from 2023. This will cause the API to stop generating new responses after the tenth response. Therefore, the value of the "stop" field would be "10" in this case. In this way, OpenAI's Chat Completion will generate a list of the top 10 movies of 2023, in order of relevance, and this will be the answer returned by your chat.|
| best_of | The purpose of this parameter is used to specify how many "best" results should be returned by the model. It is used to search for the most relevant and reliable answers for a given request. The parameter "best_of" is optional and its default value is 1, which means that the model returns only one response. If the value of "best_of" is set to a higher number, the model returns multiple answers and ranks each answer according to its quality. | Examples of using the "best_of" parameter are: <br> 1. A tech support chatbot can use the "best_of" parameter to take several answers relevant to a customer's question and rank them according to their accuracy. The chatbot can then choose the most reliable answer to provide to the customer. <br> 2. A restaurant search application can use the "best_of" parameter to get multiple restaurant recommendations based on criteria such as location, price and cuisine type. The app can then rank the recommendations according to their relevance and present the user with the best options.|
| suffix | The purpose of this parameter is to allow the OpenAI template to supplement user input based on the context provided by the suffix text. This can help improve the relevance and quality of template responses. | Examples of using the "suffix" parameter are: <br> 1. In a conversation with a virtual assistant, the user may send a text entry that describes a technical problem. The virtual assistant can use the "suffix" parameter to add more information about the user's system, or about the steps the user has already tried to solve the problem. This can help the OpenAI model better understand the context and provide a more accurate answer. <br> 2. In a text suggestion application, the user can enter an initial word or phrase and use the "suffix" parameter to provide more context or additional information. For example, if the user enters "pizza" as text input, the application can use the "suffix" parameter to provide information such as "with pepperoni" or "without cheese". This can help the model generate more accurate and relevant suggestions.|
| echo | The "echo" parameter is an option available in the OpenAI chat completion API that can be used to return the user input in the chat completion result. When the "echo" parameter is set to true, the API will include the user input in the completion result in addition to the response generated by the language model The purpose of the "echo" parameter is to provide more context and clarity for the user by helping him understand how the response was generated by the language model and allowing him to see his original input. Using the "echo" parameter can help increase the transparency and reliability of the responses generated by the OpenAI chat completion API. | Example use cases for the "echo" parameter are: <br> 1. Customer support: when using OpenAI's chat completion API to provide customer support, it can be useful to include the user input in the completion result so that the support agent can see exactly what the user asked and better understand the response generated by the model. <br> 2. Customer service chatbots: In customer service chatbots, the "echo" parameter can be used to include the user's input in the answer generated by the model, allowing the user to see their original question and to confirm that the answer is relevant and satisfactory. This can help increase the chatbot's reliability and improve the user experience.|

#### Reference

https://platform.openai.com/docs/api-reference/completions